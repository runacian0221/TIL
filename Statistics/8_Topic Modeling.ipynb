{"cells":[{"cell_type":"markdown","metadata":{"id":"_OzRnst_HJJu"},"source":["# 토픽모델링 (Topic Modeling)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Xh94rk2QHMxR"},"source":["# 0 환경"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34182,"status":"ok","timestamp":1658831462921,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"N-hrtKncHPL4","outputId":"eb76eb0c-dedb-489e-94eb-d914a925c5d7"},"outputs":[],"source":["!sudo apt-get install -y fonts-nanum\n","!sudo fc-cache -fv\n","!rm ~/.cache/matplotlib -rf\n","\n","# 런타임 재시작 !"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.rc('font', family='NanumBarunGothic')\n","plt.rcParams['axes.unicode_minus'] = False\n","\n","import warnings \n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1887,"status":"ok","timestamp":1658831464804,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"6aki-vwzBSrE","outputId":"3f1fa808-f521-4241-acf8-950443d43c25"},"outputs":[],"source":["import nltk\n","nltk.download('stopwords')"]},{"cell_type":"markdown","metadata":{"id":"Jyp5nHXQsXez"},"source":["# 1 잠재의미분석 (Latent Semantic Analysis LSA)"]},{"cell_type":"markdown","metadata":{"id":"rAQyWNVQWAh8"},"source":["## 1.1 직접 구현"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GnynjZgAM69T"},"source":["### 1) 직접 구현"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":615,"status":"ok","timestamp":1658831465416,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"MFgOmDD84TkZ"},"outputs":[],"source":["import numpy as np\n","from collections import defaultdict\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.decomposition import randomized_svd\n","\n","class LSA :\n","    def __init__(self, doc_ls, topic_num):\n","        self.doc_ls = doc_ls\n","        self.topic_num = topic_num\n","        self.term2idx, self.idx2term = self.toIdxDict(' '.join(doc_ls).split())\n","        self.doc2idx, self.idx2doc = self.toIdxDict(doc_ls)\n","        \n","        self.tdm = self.TDM(doc_ls)\n","        self.U, self.s, self.VT = self.SVD(self.tdm)\n","        \n","        \n","        self.term_mat = self.TermVectorMatrix(self.U, self.s, topic_num)\n","        self.doc_mat = self.DocVectorMatrix(self.s, self.VT, topic_num)\n","        self.term_doc_mat = self.TermDocVectorMatrix(self.U, self.s, self.VT, topic_num)\n","        \n","        \n","        self.term_sim = self.TermSimilarityMatrix(self.term_mat)\n","        self.doc_sim = self.DocSimilarityMartrix(self.doc_mat)\n","      \n","    # 리스트내 값을 index로 변환하는 dict과\n","    # index를 리스트내 값으로 변환하는 dict\n","    def toIdxDict(self, ls) :\n","        any2idx = defaultdict(lambda : len(any2idx))\n","        idx2any = defaultdict()\n","\n","        for item in ls:\n","            any2idx[item]\n","            idx2any[any2idx[item]] = item\n","            \n","        print(idx2any)\n","        return any2idx, idx2any\n","    \n","    def TDM(self, doc_ls):\n","        # 행(토큰크기), 열(문서갯수)로 TDM 생성\n","        tdm = np.zeros([len(self.term2idx.keys()), len(doc_ls)])\n","        \n","        for doc_idx, doc in enumerate(doc_ls) :\n","            for term in doc.split() :\n","              #등장한 단어를 dictionary에서 위치를 탐색하여 빈도수 세기\n","              tdm[self.term2idx[term], doc_idx] += 1\n","        self.tdm = tdm\n","        \n","        return tdm\n","    \n","    # 특이값 분해\n","    def SVD(self, tdm):\n","        U, s, VT = randomized_svd(tdm, \n","                                  n_components=self.topic_num,\n","                                  n_iter=10,\n","                                  random_state=None)\n","        \n","        U, s, VT = np.linalg.svd(tdm, full_matrices=True)\n","        return U, s, VT\n","    \n","    # 토픽별 주요 키워드 출력\n","    def TopicModeling(self, count = 3) :\n","        topic_num = self.topic_num\n","        \n","        for i in range(topic_num) :\n","            score = self.U[:,i:i+1].T\n","            print(score)\n","            sorted_index = np.flip(np.argsort(-score),0)\n","            \n","            a = []\n","            for j in sorted_index[0,: count] :\n","                a.append((self.idx2term[j], score[0,j].round(3)))\n","            \n","            print(\"Topic {} - {}\".format(i+1,a ))\n","    \n","    def vectorSimilarity(self, matrix) :\n","        similarity = np.zeros([matrix.shape[1], matrix.shape[1]])\n","        \n","        for i in range(matrix.shape[1]) :\n","            for j in range(matrix.shape[1]) :\n","                similarity[i,j] =  cosine_similarity(matrix[:,i].T, matrix[:,j].T)\n","          \n","        return similarity\n","    \n","    # 키워드를 입력했을 때 단어 벡터 반환\n","    def GetTermVector(self, term):\n","        vec = self.term_mat[self.term2idx[term]:self.term2idx[term]+1,:]\n","        print('{} = {}'.format(term, vec))\n","        return vec\n","    \n","    # 문서를 입력했을 때 문서 벡터 반환\n","    def GetDocVector(self, doc):\n","        vec = self.doc_mat.T[self.doc2idx[doc]:self.doc2idx[doc]+1,:]\n","        print('{} = {}'.format(doc, vec))\n","        return vec\n","    \n","    def Compression(self, round_num=0) :\n","        print(self.tdm)\n","        print(self.term_doc_mat.round(round_num))\n","    \n","    def TermVectorMatrix(self, u, s, topic_num):\n","        term_mat = np.matrix(u[:, :topic_num])# * np.diag(s[:topic_num])\n","        return term_mat\n","    \n","    def DocVectorMatrix(self, s, vt, topic_num):\n","        doc_mat = np.matrix(vt[:topic_num,:])\n","        return doc_mat\n","    \n","    def TermDocVectorMatrix(self, u, s, vt, topic_num):\n","        term_doc_mat = np.matrix(u[:, :topic_num]) * np.diag(s[:topic_num])  * np.matrix(vt[:topic_num,:])\n","        return term_doc_mat\n","    \n","    def TermSimilarityMatrix(self, termVectorMatrix):\n","        return self.vectorSimilarity(termVectorMatrix.T)\n","    \n","    def GetTermSimilarity(self, term1, term2):\n","        sim = self.term_sim[self.term2idx[term1], self.term2idx[term2]]\n","        print(\"({},{}) term similarity = {}\".format(term1, term2, sim))\n","        return sim \n","    \n","    def DocSimilarityMartrix(self,docVectorMatrix):    \n","        return self.vectorSimilarity(docVectorMatrix) \n","    \n","    def GetDocSimilarity(self, doc1, doc2):\n","        sim = self.doc_sim[self.doc2idx[doc1], self.doc2idx[doc2]]\n","        print(\"('{}','{}') doc similarity = {}\".format(doc1, doc2, sim))\n","        return sim "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":555,"status":"ok","timestamp":1658831465962,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"on3YdPBZp2te","outputId":"2a435e21-2a32-4c69-b103-fc70847523cd"},"outputs":[],"source":["doc_ls = [\n","    '바나나 사과 포도 포도 짜장면',\n","    '사과 포도',\n","    '포도 바나나',\n","    '짜장면 짬뽕 탕수육',\n","    '볶음밥 탕수육',\n","    '짜장면 짬뽕',\n","    '라면 스시',\n","    '스시 짜장면',\n","    '가츠동 스시 소바',\n","    '된장찌개 김치찌개 김치',\n","    '김치 된장 짜장면',\n","    '비빔밥 김치'\n","]\n","lsa = LSA(doc_ls, 4)\n","print('== 토픽 모델링 ==')\n","lsa.TopicModeling(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1658831465963,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"4jMEhMzVdBIX","outputId":"74bf478f-9883-4119-d2e9-9a4214bc3aeb"},"outputs":[],"source":["lsa.tdm"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":486,"status":"ok","timestamp":1658831466447,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"DcS6Ug9hbuAv"},"outputs":[],"source":["np.linalg.svd?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":557,"status":"ok","timestamp":1658831466985,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"JqZnr8SX86qt","outputId":"f2b11bfc-664d-4d49-f1b4-7cb6f388f552"},"outputs":[],"source":["doc_ls = [\n","    '바나나 사과 포도 포도 짜장면',\n","    '사과 포도',\n","    '포도 바나나',\n","    '짜장면 짬뽕 탕수육',\n","    '볶음밥 탕수육',\n","    '짜장면 짬뽕',\n","    '라면 스시',\n","    '스시 짜장면',\n","    '가츠동 스시 소바',\n","    '된장찌개 김치찌개 김치',\n","    '김치 된장 짜장면',\n","    '비빔밥 김치'\n","]\n","lsa = LSA(doc_ls, 4)\n","X = lsa.TDM(doc_ls)\n","print('== 토픽 모델링 ==')\n","lsa.TopicModeling(4)\n","print('\\n== 단어 벡터 ==')\n","lsa.GetTermVector('사과')\n","lsa.GetTermVector('짜장면')\n","print('\\n== 단어 유사도 ==')\n","lsa.GetTermSimilarity('사과','바나나')\n","lsa.GetTermSimilarity('사과','짜장면')\n","lsa.GetTermSimilarity('포도','짜장면')\n","lsa.GetTermSimilarity('사과','스시')\n","print('\\n== 문서 벡터 ==')\n","lsa.GetDocVector('사과 포도')\n","lsa.GetDocVector('짜장면 짬뽕')\n","print('\\n== 문서 유사도 ==')\n","lsa.GetDocSimilarity('사과 포도', '포도 바나나')\n","lsa.GetDocSimilarity('사과 포도', '라면 스시')"]},{"cell_type":"markdown","metadata":{"id":"c0Ah5QgSqEce"},"source":["### 2) 실습 템플릿"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":55,"status":"ok","timestamp":1658831466986,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"_Dsdajm39w73"},"outputs":[],"source":["class LSA :\n","    def __init__(self, doc_ls, topic_num):\n","        pass\n","    \n","    # tdm matrix 생성\n","    def TDM(self, doc_ls):\n","        pass\n","    \n","    # tdm matrix 특이값 분해(SVD)\n","    # U, s, Vt로 분해\n","    def SVD(self, tdm):\n","        pass\n","    \n","    # 토픽별 주요 키워드 출력\n","    def TopicModeling(self) :\n","        pass\n","        \n","    # 단어 벡터 행렬 생성 dot(U,s)  \n","    def TermVectorMatrix(self, u, s):\n","        pass\n","    \n","    # 문서 벡터 행렬 생성 dot(s,Vt).T \n","    def DocVectorMatrix(self, s, vt):\n","        pass\n","    \n","    # 키워드를 입력했을 때 단어 벡터 반환\n","    def GetTermVector(self, term):\n","        pass\n","    \n","    # 문서를 입력했을 때 문서 벡터 반환\n","    def GetDocVector(self, doc):\n","        pass\n","    \n","    # 단어-문서 벡터 행렬 생성\n","    def TermDocVectorMatrix(self, u, s, vt):\n","        pass\n","    \n","    # 단어 벡터 행렬에서 단어 간 코사인 유사도 측정하여 행렬형태로 반환\n","    def TermSimilarityMatrix(self, term_vec_matrix):\n","        pass\n","    \n","    # 두개 단어를 입력했을 때 코사인 유사도 반환\n","    def GetTermSimilarity(self, term1, term2):\n","        pass\n","    \n","    # 문서 벡터 행렬에서 문서 간 코사인 유사도 측정하여 행렬형태로 반환\n","    def DocSimilarityMartrix(self, doc_vec_matrix):\n","        pass\n","    \n","    # 두개 문서를 입력했을 때 코사인 유사도 반환\n","    def GetDocSimilarity(self, doc1, doc2):\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":55,"status":"ok","timestamp":1658831466986,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"wHB9A2VjGE0j"},"outputs":[],"source":["doc_ls = [\n","    '바나나 사과 포도 포도',\n","    '사과 포도',\n","    '포도 바나나',\n","    '짜장면 짬뽕 탕수욕',\n","    '볶음밥 탕수욕',\n","    '짜장면 짬뽕',\n","    '라면 스시',\n","    '스시',\n","    '가츠동 스시 소바',\n","    '된장찌개 김치찌개 김치',\n","    '김치 된장',\n","    '비빔밥 김치'\n","]\n","\n","lsa = LSA(doc_ls, 3)\n","lsa.TopicModeling()\n","lsa.GetTermSimilarity('사과','바나나')\n","lsa.GetTermSimilarity('사과','짜장면')\n","lsa.GetDocSimilarity('사과 포도', '포도 바나나')\n","lsa.GetDocSimilarity('사과 포도', '라면 스시')"]},{"cell_type":"markdown","metadata":{"id":"31Kn3iEWLDz9"},"source":["## 1.2 sklearn 활용"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kaaw49O5mqec"},"source":["### 1) 토픽모델링"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":55,"status":"ok","timestamp":1658831466986,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"2LVVXaR4EJCZ"},"outputs":[],"source":["doc_ls = [\n","    '바나나 사과 포도 포도 짜장면',\n","    '사과 포도',\n","    '포도 바나나',\n","    '짜장면 짬뽕 탕수육',\n","    '볶음밥 탕수육',\n","    '짜장면 짬뽕',\n","    '라면 스시',\n","    '스시 짜장면',\n","    '가츠동 스시 소바',\n","    '된장찌개 김치찌개 김치',\n","    '김치 된장 짜장면',\n","    '비빔밥 김치'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55,"status":"ok","timestamp":1658831466987,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"ZOPy5VKzKr8_","outputId":"3e196518-540a-4d25-f170-3ce4cf29fad1"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","\n","n_topic= 4\n","\n","tfidf_vect = TfidfVectorizer(max_features= 1000, max_df = 0.5, smooth_idf=True)\n","tfidf = tfidf_vect.fit_transform(doc_ls)\n","svd = TruncatedSVD(n_components=n_topic, algorithm='randomized', n_iter=100)\n","u_sigma = svd.fit_transform(tfidf)\n","svd.components_"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":519,"status":"ok","timestamp":1658831467500,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"fkHor8JeRuFz","outputId":"18da964b-4c25-40d2-8f8a-2c131b9eb904"},"outputs":[],"source":["vocab = tfidf_vect.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n","n = 3\n","for idx, topic in enumerate(svd.components_):\n","    print(\"Topic %d:\" % (idx), [(vocab[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Cj3fMxdamzB6"},"source":["### 2) 단어벡터"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658831467501,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"jQvD_3Lagat8","outputId":"8f454c73-c589-464f-ce79-61bdcb52b22d"},"outputs":[],"source":["svd.components_.T"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1658831467501,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"C2VjqPqSkOk4","outputId":"0608fc87-26c0-4ffa-d76c-e627fe58f023"},"outputs":[],"source":["# 단어벡터\n","for i in range(len(vocab)) :\n","    print(\"{} : {}\".format(vocab[i], svd.components_.T[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1658831467501,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"pdzmkhP8ELav"},"outputs":[],"source":["import numpy as np\n","from numpy import dot\n","from numpy.linalg import norm\n","\n","def calc_similarity_matrix(vectors) :\n","    def cosine_similarity(a, b) : \n","        return dot(a, b)/(norm(a)*norm(b))\n","\n","    n_word = len(vectors)\n","    similarity_matrix = np.zeros((n_word, n_word))\n","\n","    for i in range(n_word) :\n","        for j in range(i, n_word) :\n","            similarity_matrix[j, i] = cosine_similarity(vectors[i], vectors[j]).round(4)  \n","\n","    return similarity_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":858,"status":"ok","timestamp":1658831468356,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"QzEGCRDzB_CU"},"outputs":[],"source":["import seaborn as sns\n","\n","def visualize_similarity(similarity_matrix) :\n","    uniform_data = similarity_matrix\n","    mask = np.triu(np.ones_like(similarity_matrix, dtype=np.bool))\n","    plt.rcParams['figure.figsize'] = [8, 6]\n","    ax = sns.heatmap(uniform_data, mask=mask, #xticklabels=features, yticklabels=features, \n","                    annot=True, fmt=\".2f\",annot_kws={'size':8}, cmap='coolwarm')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"executionInfo":{"elapsed":2592,"status":"ok","timestamp":1658831470945,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"MWJI_8rbBzAL","outputId":"cdfd3462-875a-4972-9799-24b26621ed05"},"outputs":[],"source":["word_vectors = svd.components_.T\n","word_similarity_matrix = calc_similarity_matrix(word_vectors)\n","visualize_similarity(word_similarity_matrix)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0yfe991tm1qa"},"source":["### 3) 문서벡터"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453},"executionInfo":{"elapsed":916,"status":"ok","timestamp":1658831471854,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"49wUP0VdkQVD","outputId":"ec9a4508-6844-4be8-9b88-a7659b9ea3c8"},"outputs":[],"source":["doc_vectors = u_sigma/svd.singular_values_\n","doc_similarity_matrix = calc_similarity_matrix(doc_vectors)\n","visualize_similarity(doc_similarity_matrix)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uGFHkelP2Upp"},"source":["### 4) 벡터 시각화\n","\n","- manifold.TSNE() : t-SNE(t분포 Stochastic Neighbor Embedding) 차원 축소 기법의 하나"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1658831471854,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"dUl3WoSi2vw3","outputId":"fd809f48-eb09-4acc-98c6-5a0ee5c0296d"},"outputs":[],"source":["vectors = word_vectors\n","labels = tfidf_vect.get_feature_names()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":533,"status":"ok","timestamp":1658831472383,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"XVezoaECoPzO"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","import numpy as np\n","\n","def visualize_vectors(vectors, labels):\n","    tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n","    np.set_printoptions(suppress=True)\n","    T = tsne.fit_transform(vectors)\n","    #labels = vocab\n","\n","    plt.figure(figsize=(10, 6))\n","    plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n","    for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n","        plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6020,"status":"ok","timestamp":1658831478396,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"Ufmu2SzjDWJj","outputId":"65ffc4cc-e7c9-473f-defc-d3df0e84a40f"},"outputs":[],"source":["visualize_vectors(vectors, labels)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4EpgqX0rVPjM"},"source":["### 5) 파이프라인 사용"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":458,"status":"ok","timestamp":1658831548920,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"qn6YnSohVTdo","outputId":"0aa6c489-6c45-4d28-c11b-c76bf563a08b"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","\n","def my_tokenizer(text):\n","    return [w for w in text.split() if len(w) > 1]\n","             \n","lsa_pipeline = Pipeline([\n","    ('vect', CountVectorizer(tokenizer = my_tokenizer)),\n","    ('tfidf', TfidfTransformer(smooth_idf=True)),\n","    ('lsa', TruncatedSVD(n_components=n_topic, algorithm='randomized', n_iter=100)), \n","])\n","\n","lsa_pipeline.fit(doc_ls)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":436,"status":"ok","timestamp":1658831552466,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"RJjRa7wictNS"},"outputs":[],"source":["lsa = lsa_pipeline.named_steps['lsa']\n","count_vect = lsa_pipeline.named_steps['vect']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":547,"status":"ok","timestamp":1658831553614,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"XYzHYwd_cxEh","outputId":"f473c253-0613-4c28-d201-9d8e300ce9b0"},"outputs":[],"source":["vocab = count_vect.get_feature_names() # 단어 집합. 1,000개의 단어가 저장됨.\n","\n","def get_topics(components, feature_names, n=3):\n","    for idx, topic in enumerate(components):\n","        print(\"Topic %d:\" % (idx), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n","get_topics(lsa.components_,vocab)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QiJXdInJLn-4"},"source":["## 1.3 gensim 활용"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"q5R7Z3VULtQT"},"source":["### 1) 토픽모델링"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1658831557210,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"lz82ZZpkLr9I"},"outputs":[],"source":["docs = [\n","    '바나나 사과 포도 포도',\n","    '사과 포도',\n","    '포도 바나나',\n","    '짜장면 짬뽕 탕수욕',\n","    '볶음밥 탕수욕',\n","    '짜장면 짬뽕',\n","    '라면 스시',\n","    '스시',\n","    '가츠동 스시 소바',\n","    '된장찌개 김치찌개 김치',\n","    '김치 된장',\n","    '비빔밥 김치'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1658831557210,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"JjZIWeWuL1NT","outputId":"b0d90b38-cad1-4783-9ca1-d88c4bca0b52"},"outputs":[],"source":["doc_ls = [doc.split() for doc in docs]\n","doc_ls[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":749,"status":"ok","timestamp":1658831557957,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"eCcg7TSHL5ZR","outputId":"0af6b4c6-dd31-499b-e94a-a365f187c9fd"},"outputs":[],"source":["from gensim import corpora\n","from gensim.models import LsiModel\n","from gensim.models import TfidfModel\n","\n","n_dim = 4\n","\n","id2word = corpora.Dictionary(doc_ls)\n","corpus_TDM = [id2word.doc2bow(text) for text in doc_ls]\n","tfidf = TfidfModel(corpus_TDM) #train\n","corpus_TFIDF = tfidf[corpus_TDM] #predict\n","model_LSA = LsiModel(corpus_TFIDF, id2word=id2word, num_topics=n_dim)\n","\n","for top in model_LSA.print_topics(n_dim, 3):\n","    print(top)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1658831557957,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"q-kX7e6Hm38H","outputId":"c4d3ab95-6e86-4899-ead5-282752606703"},"outputs":[],"source":["model_LSA.projection.u"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1658831557958,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"QinBYNCcndqZ","outputId":"83197dc8-2023-4433-b8e6-7f90c14ff329"},"outputs":[],"source":["from gensim.matutils import sparse2full\n","corpus_VT = model_LSA[corpus_TDM]\n","VT = [sparse2full(doc_vector, n_dim).tolist() for doc_vector in corpus_VT]\n","VT"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OcR4bsoaL_V-"},"source":["### 2) 단어벡터"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1658831557958,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"Qe2_eReNMRta","outputId":"339a951b-bd13-4f75-fb2c-a525eff42f28"},"outputs":[],"source":["for i in id2word.keys() :\n","    print(\"{} : {}\".format(id2word[i], model_LSA.projection.u[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453},"executionInfo":{"elapsed":2043,"status":"ok","timestamp":1658831559998,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"tvsgIQEOC2Fv","outputId":"1d729847-2048-4ba7-a431-2a61b16982e8"},"outputs":[],"source":["word_vectors = model_LSA.projection.u\n","word_similarity_matrix = calc_similarity_matrix(word_vectors)\n","visualize_similarity(word_similarity_matrix)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EAX85JKnMDOR"},"source":["### 3) 문서벡터"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453},"executionInfo":{"elapsed":1020,"status":"ok","timestamp":1658831561015,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"vwLmgBaBMZmQ","outputId":"549d51b1-13b7-4838-f4e5-ecd002d12ccc"},"outputs":[],"source":["from gensim.matutils import sparse2full\n","corpus_V = model_LSA[corpus_TDM]\n","V = [sparse2full(doc_vector, n_dim).tolist() for doc_vector in corpus_VT]\n","\n","doc_vectors = V\n","doc_similarity_matrix = calc_similarity_matrix(doc_vectors)\n","visualize_similarity(doc_similarity_matrix)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"l-nbVVMzMGEo"},"source":["### 4) 벡터시각화"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"executionInfo":{"elapsed":687,"status":"ok","timestamp":1658831561698,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"7AA98BMjDdpn","outputId":"d1a436b6-65cf-4539-dc28-72eebe5011b9"},"outputs":[],"source":["vectors = word_vectors\n","labels = [k for k in id2word.keys()]\n","\n","visualize_vectors(vectors, labels)"]},{"cell_type":"markdown","metadata":{"id":"ATfiKPg2wHkQ"},"source":["# 2 잠재디리클레할당(LDA, Latent Dirichlet Allocation)"]},{"cell_type":"markdown","metadata":{"id":"kTdsTBi5qO6x"},"source":["## 2.1 직접구현"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"adoVt_1RNJB6"},"source":["### 1) 직접 구현"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658831561698,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"Qxl6Bat79QQk"},"outputs":[],"source":["import random\n","import numpy as np\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from collections import defaultdict\n","\n","class LDA :\n","    def __init__(self, docs, topic_num, alpha = 0.1, beta = 0.001):\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.k = topic_num\n","        self.docs = docs\n","      \n","    def RandomlyAssignTopic(self, docs) :\n","        dic = defaultdict()\n","        t2i = defaultdict(lambda : len(t2i))\n","        i2t = defaultdict()\n","        d = 0\n","        w = 0\n","      \n","        wnl = WordNetLemmatizer()\n","        stopword = stopwords.words('english')\n","        stopword.append(',')\n","        \n","        # 임의의 토픽을 할당\n","        for tokens in [word_tokenize(doc) for doc in docs] :\n","            for token in [wnl.lemmatize(token.lower()) for token in tokens \n","                          if token not in stopword] :\n","                i2t[t2i[token]] = token\n","                dic[(d, t2i[token], w)] = random.randint(0,self.k-1)\n","                w += 1\n","            d += 1\n","        \n","        print(dic)\n","        return dic, t2i, i2t\n","    \n","    \n","    def CountDocTopic(self, t2i) :\n","        docs = np.zeros((self.k, len(self.docs)))\n","        terms = np.zeros((self.k, len(t2i.keys())))\n","\n","        \n","        #문서별 토큰별 빈도수 계산\n","        for (d, n, w) in self.term_topic.keys() :\n","            topic = self.term_topic[(d, n, w)]\n","            docs[topic, d] += 1 + self.alpha\n","            terms[topic, n] += 1 + self.beta\n","        \n","        #비어있는 값는 값에 alpha, beta 설정\n","        docs = np.where(docs==0.0, self.alpha, docs) \n","        terms = np.where(terms==0.0, self.beta, terms)\n","        \n","        print(\"단어 토픽별 빈도\")\n","        print(terms.round(2))\n","        print(\"문서 토픽별 빈도\")\n","        print(docs.round(1))\n","          \n","        return docs, terms\n","    \n","    \n","    def IterateAssignTopic(self, docs, terms, i2t) :\n","        #한개의 단어씩 주제 배정\n","        prev = {}\n","        \n","        while prev != self.term_topic:\n","            for (d, n, w) in self.term_topic.keys() :\n","                topic = [0, 0]\n","\n","                docs[self.term_topic[(d, n, w)], d] -= (1 + self.alpha) #숫자만 빼야함. 코드 실행하여 재확인\n","                terms[self.term_topic[(d, n, w)], n] -= (1 + self.beta)\n","                \n","                docs = np.where(docs==0.0, self.alpha, docs) \n","                terms = np.where(terms==0.0, self.beta, terms)\n","\n","                print()\n","                print(\"{}(d:{}, n:{}, w:{}) = topic:{}\".format(i2t[n], d, n, w, self.term_topic[(d, n, w)]))\n","                print(\"문서 토픽별 빈도\")\n","                print(docs.round(1))\n","                print(\"단어 토픽별 빈도\")\n","                print(terms.round(3))\n","                \n","                \n","                prev = self.term_topic\n","                \n","                for t in range(self.k) :\n","                    p_t_d = docs[t, d]/docs[:,d:d+1].sum()\n","                    p_w_t = terms[t, n]/terms[t:t+1,:].sum()\n","                    prob = p_t_d * p_w_t\n","\n","                    if topic[1] < prob :\n","                        topic = [t, prob]\n","                      \n","                    print(\"topic {} 일 확률 = {}/{} * {}/{} = {} * {} = {}\".format(t\n","                        ,docs[t, d].round(1) , docs[:,d:d+1].sum().round(1)\n","                        ,terms[t, d].round(3) , terms[t:t+1,:].sum().round(3)\n","                        , p_t_d.round(3), p_w_t.round(4), prob.round(4)))\n","                \n","                \n","                if docs[topic[0], d] < 1 : docs[topic[0], d] = 0\n","                if terms[topic[0], n] < 1 : terms[topic[0], n] = 0\n","                  \n","                #확률이 가장 큰 토픽을 할당  \n","                self.term_topic[(d, n, w)] = topic[0]\n","                docs[topic[0], d] += (1 + self.alpha)\n","                terms[topic[0], n] += (1 + self.beta)\n","                \n","                print(\"할당된 토픽:{}\".format(self.term_topic[(d, n, w)]))\n","                print(\"=\"*50)\n","            \n","        return terms\n","\n","    \n","    # 토픽별 주요 키워드 출력\n","    def TopicModeling(self, count=3) :\n","        self.term_topic, t2i, i2t = self.RandomlyAssignTopic(self.docs)\n","        docs, terms = self.CountDocTopic(t2i)\n","        terms = self.IterateAssignTopic(docs, terms, i2t)\n","        \n","        score = terms / terms.sum(axis=1, keepdims=True)\n","        \n","        for i in range(self.k) :\n","            print(\"\\nTopic {}\".format(i+1))\n","            sorted_index = np.flip(np.argsort(score[i]),0)[:count]\n","            for j in sorted_index :\n","                #pass\n","                print(\"({}={})\".format(i2t[j], score[i,j].round(3)), end = ' ')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2711,"status":"ok","timestamp":1658831619111,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"-gIJ4GNhEW0v","outputId":"13d98540-c9d7-45be-c1fe-8d8801dbfdc4"},"outputs":[],"source":["nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2853,"status":"ok","timestamp":1658831621959,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"0sfF7Quwrv9m","outputId":"a06ecfa3-92ac-4bac-b3d8-70c8e158e949"},"outputs":[],"source":["doc_ls = [\n","    \"Cute kitty\",\n","    \"Eat rice or cake\",\n","    \"Kitty and hamster\",\n","    \"Eat bread\",\n","    \"Rice, bread and cake\",\n","    \"Cute hamster eats bread and cake\"\n","]\n","\n","lda = LDA(doc_ls, 2)\n","lda.TopicModeling(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"33en4O98NDdT"},"source":["### 2) 실습 템플릿"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":582,"status":"ok","timestamp":1658831628079,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"O3F7SDbJsrve"},"outputs":[],"source":["class LDA :\n","    def __init__(self, doc_ls, topic_num, alpha = 0.1, beta = 0.001):\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.k = topic_num\n","    \n","    def RandomlyAssignTopic(self, doc_ls):\n","        pass\n","    \n","\n","    def IterateAssignTopic(self) :\n","        pass\n","    \n","    # 토픽별 주요 키워드 출력\n","    def TopicModeling(self) :\n","        pass"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RS7TS1QP9Q7e"},"source":["## 2.2  sklearn 활용"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vwuy0ZcyNhfa"},"source":["### 1) 토픽모델링 (파이프라인 미사용)\n","\n","- decomposition.LatentDirichletAllocation() : LDA 모델"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20293,"status":"ok","timestamp":1658831648369,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"pX2qNArwVPbY","outputId":"2f25597c-7585-45e8-c722-2f9132baeca2"},"outputs":[],"source":["!pip install pyLDAvis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":439,"status":"ok","timestamp":1658831648802,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"SaZrFgPaOuqJ","outputId":"f6d612d1-5323-467b-e6ad-b0de11e0761c"},"outputs":[],"source":["import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","import pandas as pd\n","from sklearn.datasets import fetch_20newsgroups\n","\n","#뉴스 다운로드 및 전처리\n","def get_news(apply_split=True) :\n","    #20newsgroup 다운로드\n","    dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n","    documents = dataset.data\n","\n","    news_df = pd.DataFrame({'document':documents})\n","    news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # 특수 문자 제거\n","    news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3])) # 길이가 3이하인 단어는 제거 (길이가 짧은 단어 제거)\n","    news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())# 전체 단어에 대한 소문자 변환\n","    tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) # 토큰화\n","\n","    stop_words = stopwords.words('english') # NLTK 불용어 조회\n","\n","    if apply_split :\n","        return tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n","    else :\n","        return tokenized_doc.apply(lambda x: ' '.join([item for item in x if item not in stop_words]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36143,"status":"ok","timestamp":1658831684941,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"cz82Rew1s9kF","outputId":"4764caaa-2523-49ae-91eb-6a4394840366"},"outputs":[],"source":["#공백으로 토큰 분리\n","def my_tokenizer(text):\n","    return text.split()\n","\n","tokenized_docs = get_news(False)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":269020,"status":"ok","timestamp":1658831953950,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"ZIuciWdLLeuf"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","tfidf_vect = TfidfVectorizer(tokenizer = my_tokenizer)\n","tfidf = tfidf_vect.fit_transform(tokenized_docs)\n","lda = LatentDirichletAllocation(n_components=20, \n","                                max_iter=20, \n","                                learning_method='online', \n","                                random_state=100)\n","\n","lda_output = lda.fit_transform(tfidf)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":11903,"status":"ok","timestamp":1658831965847,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"oyAE2FJjI1CV","outputId":"fe8fe191-a3f1-40d0-9266-7f90b9261052"},"outputs":[],"source":["#!pip install pyLDAvis\n","import pyLDAvis\n","import pyLDAvis.sklearn\n","\n","pyLDAvis.enable_notebook()\n","vis = pyLDAvis.sklearn.prepare(lda, tfidf, tfidf_vect, mds='tsne')\n","pyLDAvis.display(vis)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"iTfiF5E7VDk3"},"source":["### 2) 토픽모델링 (파이프라인 사용)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":266841,"status":"ok","timestamp":1658832232684,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"ppgErMneHMhs","outputId":"01eb99f3-27e9-4532-8917-cfb7af302e13"},"outputs":[],"source":["from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","tokenized_docs = get_news(False)\n","#파이프라이구성\n","lda_pipeline = Pipeline([\n","    ('tfidf_vect', TfidfVectorizer(tokenizer = my_tokenizer)),\n","    ('lda', LatentDirichletAllocation(n_components=20, \n","                                      max_iter=20, \n","                                      learning_method='online', \n","                                      random_state=100))])\n","\n","lda_pipeline.fit(tokenized_docs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2987,"status":"ok","timestamp":1658832235667,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"hPcr3FOGtj3Z","outputId":"74c5eee0-3963-4410-8553-5263bdb3eda4"},"outputs":[],"source":["!pip install pyLDAvis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":15891,"status":"ok","timestamp":1658832251555,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"tfenOQXhU1q_","outputId":"e141d80b-78f2-4b10-c45f-70bdb9ce07c1"},"outputs":[],"source":["#!pip install pyLDAvis\n","import pyLDAvis\n","import pyLDAvis.sklearn\n","\n","tfidf_vect = lda_pipeline.named_steps['tfidf_vect']\n","tfidf = tfidf_vect.fit_transform(tokenized_docs)\n","lda = lda_pipeline.named_steps['lda']\n","\n","vis = pyLDAvis.sklearn.prepare(lda, tfidf, tfidf_vect, mds='tsne')\n","pyLDAvis.display(vis)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-Cdpwhk7N80Y"},"source":["### 3) 하이퍼파라미터 튜닝"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1685256,"status":"ok","timestamp":1658833936808,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"IxA8XpMQHLHF","outputId":"cf00639e-6947-4cef-b6d9-02a418195bc8"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","def tuning_hyperparams(train_data, n_jobs=1) :                \n","    lda_pipeline = Pipeline([\n","        ('tfidf_vect', TfidfVectorizer(tokenizer = my_tokenizer)),\n","        ('lda', LatentDirichletAllocation(max_iter=10, random_state=100))\n","    ])\n","    \n","    search_params = {\n","        'tfidf_vect__ngram_range': [(1, 1), (1, 2)],\n","        'tfidf_vect__use_idf': (True, False),\n","        'lda__n_components': [10, 20]\n","    }\n","\n","    gs_lda = GridSearchCV(lda_pipeline, search_params, n_jobs=n_jobs)\n","    gs_lda = gs_lda.fit(train_data)\n","    print(\"Best score: {0}\".format(gs_lda.best_score_))  \n","    print(\"Best parameters set:\")  \n","    best_parameters = gs_lda.best_estimator_.get_params()  \n","    for param_name in sorted(list(best_parameters.keys())):  \n","        print(\"\\t{0}: {1}\".format(param_name, best_parameters[param_name]))\n","    return gs_lda.best_estimator_\n","\n","lda_pipeline = tuning_hyperparams(tokenized_docs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":9595,"status":"ok","timestamp":1658833946401,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"x16CyCWMXj8R","outputId":"5afb9ff1-6c9d-4fa6-db06-fe7166573eab"},"outputs":[],"source":["#!pip install pyLDAvis\n","import pyLDAvis\n","import pyLDAvis.sklearn\n","\n","tfidf_vect = lda_pipeline.named_steps['tfidf_vect']\n","tfidf = tfidf_vect.fit_transform(tokenized_docs)\n","lda = lda_pipeline.named_steps['lda']\n","\n","vis = pyLDAvis.sklearn.prepare(lda, tfidf, tfidf_vect, mds='tsne')\n","pyLDAvis.display(vis)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xf_J97e1NsTl"},"source":["## 2.3 gensim 활용"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mbB7yMnRNvB6"},"source":["### 1) 토픽모델링"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1658833946401,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"c0hifkL58rrB"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21309,"status":"ok","timestamp":1658833967708,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"r-FAziaL9KJ6","outputId":"fb8553a1-2773-440a-fdbf-335ef4540cf8"},"outputs":[],"source":["from gensim import corpora\n","from gensim.models import LdaModel, TfidfModel\n","\n","tokenized_docs = get_news()\n","id2word = corpora.Dictionary(tokenized_docs)\n","corpus_TDM = [id2word.doc2bow(doc) for doc in tokenized_docs]\n","tfidf = TfidfModel(corpus_TDM)\n","corpus_TFIDF = tfidf[corpus_TDM]\n","\n","n = 20\n","lda = LdaModel(corpus=corpus_TFIDF,\n","               id2word=id2word,\n","               num_topics=n, \n","               random_state=100)\n","\n","for t in lda.print_topics():\n","    print(t)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1172,"status":"ok","timestamp":1658833968872,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"shGD7kDtDF7_"},"outputs":[],"source":["corpus_TDM = [id2word.doc2bow(doc) for doc in tokenized_docs]\n","tfidf = TfidfModel(corpus_TDM)\n","corpus_TFIDF = tfidf[corpus_TDM]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":860},"executionInfo":{"elapsed":36750,"status":"ok","timestamp":1658835218083,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"42GDOBDWYR0D","outputId":"4addb638-2009-4c41-87da-f03faf1aeae0"},"outputs":[],"source":["import pyLDAvis\n","import pyLDAvis.gensim_models as gensimvis\n","\n","pyLDAvis.enable_notebook()\n","vis = gensimvis.prepare(lda, corpus_TFIDF, id2word, mds='tsne')\n","pyLDAvis.display(vis)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jJ_1fTwuN4Vd"},"source":["### 2) 하이퍼파라미터 튜닝\n","\n","- models.coherencemodel.CoherenceModel() : LDA에서 최적 토픽 개수를 추출하는 모델 (토픽의 응집력 계산)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1658835218084,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"lc9UOlhKPDj8"},"outputs":[],"source":["from gensim import corpora\n","from gensim.models import LdaModel, TfidfModel\n","from gensim.models.coherencemodel import CoherenceModel\n","\n","def compute_coherence_perplexity(tokenized_docs, end, start=2, step=3) :\n","    id2word = corpora.Dictionary(tokenized_docs)\n","    corpus_TDM = [id2word.doc2bow(doc) for doc in tokenized_docs]\n","    tfidf = TfidfModel(corpus_TDM)\n","    corpus_TFIDF = tfidf[corpus_TDM]\n","    \n","    coherence_values = []\n","    perplexity_values = []\n","    model_list = []\n","    topic_n_list = []\n","    \n","    for num_topics in range(start, end, step):\n","        model = LdaModel(corpus_TFIDF, num_topics=num_topics, id2word = id2word) \n","            \n","        model_list.append(model)\n","        coherencemodel = CoherenceModel(model=model, \n","                                        texts=tokenized_docs, \n","                                        dictionary=id2word, \n","                                        coherence='c_v')\n","        coherence_values.append(coherencemodel.get_coherence())\n","        perplexity_values.append(model.log_perplexity(corpus_TFIDF))\n","        topic_n_list.append(num_topics)\n","\n","    for t, c, p in zip(topic_n_list, coherence_values, perplexity_values) :\n","        print(\"topic_n={}, coherence : {}, perplexity : {}\".format(t,c,p))\n","\n","    return corpus_TFIDF, id2word, model_list, coherence_values, perplexity_values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":85047,"status":"ok","timestamp":1658835303121,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"rG0tNlbE3dTy","outputId":"4d71c8cf-cc74-48b5-e2b9-9f6f603ace8e"},"outputs":[],"source":["from gensim import corpora\n","from gensim.models import LdaModel, TfidfModel\n","\n","tokenized_docs = get_news()\n","corpus, id2word, model_list, coherence_values, perplexity_values = compute_coherence_perplexity(tokenized_docs, start=10, end=30, step=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1658835303121,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"4IRszRST5FZe"},"outputs":[],"source":["lda_model = model_list[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":860},"executionInfo":{"elapsed":16352,"status":"ok","timestamp":1658835373803,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"LyTD0xfM-vdI","outputId":"ed6deaa7-f75b-403d-f72a-c8fc50a054c9"},"outputs":[],"source":["import pyLDAvis\n","import pyLDAvis.gensim_models as gensimvis\n","\n","pyLDAvis.enable_notebook()\n","vis = gensimvis.prepare(lda_model, corpus, id2word, mds='tsne')\n","pyLDAvis.display(vis)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1658835373832,"user":{"displayName":"최재진","userId":"14887547996178540700"},"user_tz":-540},"id":"DcIVqBHFg9WB"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"8_Topic Modeling.ipynb","provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13"},"vscode":{"interpreter":{"hash":"e508c96042fd7b3aa969c1a8875668ac50b0a6c54de6b2bab6d59ac763cd3db2"}}},"nbformat":4,"nbformat_minor":0}
